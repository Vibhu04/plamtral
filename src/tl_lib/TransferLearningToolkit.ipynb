{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yCBJm4WzUbiI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669475684872,"user_tz":-330,"elapsed":19496,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"}},"outputId":"cecbcc4f-113a-4866-f067-65adf2695959"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kDLPVwGVa7Y","executionInfo":{"status":"ok","timestamp":1669475684873,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"}},"outputId":"0d75892f-0236-4836-8c5d-a9b5f23e3115"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/TransferLearningToolkit\n"]}],"source":["%cd gdrive/MyDrive/TransferLearningToolkit/"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"J9YSUnj0phJD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669475697098,"user_tz":-330,"elapsed":12235,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"}},"outputId":"30e5b5b7-c6cc-4bc7-d7d3-c508a18f07cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 27.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 73.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 32.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.10.3\n"]}],"source":["!pip install transformers\n","!pip install torchmetrics"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":168437,"status":"error","timestamp":1669472267559,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"},"user_tz":-330},"id":"xBiSqhrbC_J_","outputId":"9fa529ac-93a7-467a-8a1f-4fd24219b03e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logs deleted\n","Existing model loaded\n","EPOCH 0 started==============================\n","[1,     1] train loss: 6.80513\n","[1,     2] train loss: 6.14021\n","[1,     3] train loss: 6.45855\n","[1,     4] train loss: 6.61598\n","[1,     5] train loss: 6.48633\n","[1,     6] train loss: 6.54489\n","[1,     7] train loss: 6.24736\n","[1,     8] train loss: 6.17829\n","[1,     9] train loss: 6.12566\n","[1,    10] train loss: 6.17078\n","[1,    11] train loss: 6.29748\n","[1,    12] train loss: 6.31184\n","[1,    13] train loss: 6.29710\n","[1,    14] train loss: 7.23446\n","[1,    15] train loss: 6.38933\n","[1,    16] train loss: 6.09216\n","[1,    17] train loss: 6.63264\n","[1,    18] train loss: 7.10049\n","[1,    19] train loss: 6.36003\n","[1,    20] train loss: 7.40306\n","[1,    21] train loss: 5.92455\n","[1,    22] train loss: 6.33087\n","[1,    23] train loss: 7.02718\n","[1,    24] train loss: 6.50044\n","[1,    25] train loss: 6.14208\n","[1,    26] train loss: 7.14556\n","[1,    27] train loss: 6.16325\n","[1,    28] train loss: 6.31783\n","[1,    29] train loss: 6.25117\n","[1,    30] train loss: 6.54094\n","[1,    31] train loss: 6.29799\n","[1,    32] train loss: 6.25960\n","[1,    33] train loss: 6.43245\n","[1,    34] train loss: 6.06800\n","[1,    35] train loss: 6.22666\n","[1,    36] train loss: 6.08818\n","[1,    37] train loss: 6.14783\n","[1,    38] train loss: 6.15580\n","[1,    39] train loss: 6.14448\n","[1,    40] train loss: 6.16050\n","[1,    41] train loss: 6.50693\n","[1,    42] train loss: 6.37479\n","[1,    43] train loss: 6.32206\n","[1,    44] train loss: 6.17839\n","[1,    45] train loss: 6.41075\n","[1,    46] train loss: 6.46366\n","[1,    47] train loss: 6.46027\n","[1,    48] train loss: 6.15857\n","[1,    49] train loss: 6.44629\n","[1,    50] train loss: 6.26385\n","[1,    51] train loss: 6.24665\n","[1,    52] train loss: 6.28045\n","[1,    53] train loss: 6.30509\n","[1,    54] train loss: 6.83247\n","[1,    55] train loss: 6.05017\n","[1,    56] train loss: 6.60875\n","[1,    57] train loss: 7.31522\n","[1,    58] train loss: 6.62258\n","[1,    59] train loss: 7.58549\n","[1,    60] train loss: 6.33029\n","[1,    61] train loss: 6.55641\n","[1,    62] train loss: 6.20908\n","[1,    63] train loss: 6.51031\n","[1,    64] train loss: 6.28301\n","[1,    65] train loss: 6.45988\n","[1,    66] train loss: 5.90640\n","[1,    67] train loss: 6.60943\n","[1,    68] train loss: 6.20561\n","[1,    69] train loss: 6.16577\n","[1,    70] train loss: 7.26980\n","[1,    71] train loss: 6.14509\n","[1,    72] train loss: 6.36194\n","[1,    73] train loss: 6.25799\n","[1,    74] train loss: 6.40545\n","[1,    75] train loss: 6.34818\n","[1,    76] train loss: 6.76166\n","[1,    77] train loss: 7.05679\n","[1,    78] train loss: 5.94270\n","[1,    79] train loss: 6.75927\n","[1,    80] train loss: 6.32118\n","[1,    81] train loss: 6.91787\n","[1,    82] train loss: 6.20667\n","[1,    83] train loss: 6.39816\n","[1,    84] train loss: 6.10878\n","[1,    85] train loss: 6.16119\n","[1,    86] train loss: 6.55047\n","[1,    87] train loss: 6.53978\n","[1,    88] train loss: 6.49835\n","[1,    89] train loss: 7.20160\n","[1,    90] train loss: 6.20340\n","[1,    91] train loss: 6.67167\n","[1,    92] train loss: 6.41696\n","[1,    93] train loss: 6.19560\n","[1,    94] train loss: 5.89043\n","[1,    95] train loss: 6.17240\n","[1,    96] train loss: 6.47863\n","[1,    97] train loss: 6.25741\n","[1,    98] train loss: 6.04890\n","[1,    99] train loss: 6.09317\n","[1,   100] train loss: 6.41504\n","In Validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 90/90 [00:10<00:00,  8.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[1       ] validation loss: 6.39170\n","Time elapsed: 0:00:39.274829\n","[1,   101] train loss: 6.08183\n","[1,   102] train loss: 6.43237\n","[1,   103] train loss: 6.14569\n","[1,   104] train loss: 5.96955\n","[1,   105] train loss: 7.21087\n","[1,   106] train loss: 6.77217\n","[1,   107] train loss: 7.26424\n","[1,   108] train loss: 6.13122\n","[1,   109] train loss: 6.29244\n","[1,   110] train loss: 6.35110\n","[1,   111] train loss: 6.13658\n","[1,   112] train loss: 6.28882\n","[1,   113] train loss: 6.88324\n","[1,   114] train loss: 7.12477\n","[1,   115] train loss: 6.65692\n","[1,   116] train loss: 7.36114\n","[1,   117] train loss: 6.38256\n","[1,   118] train loss: 6.03209\n","[1,   119] train loss: 6.56385\n","[1,   120] train loss: 6.37020\n","[1,   121] train loss: 7.01341\n","[1,   122] train loss: 6.41341\n","[1,   123] train loss: 6.29044\n","[1,   124] train loss: 6.23307\n","[1,   125] train loss: 6.08246\n","[1,   126] train loss: 7.41094\n","[1,   127] train loss: 6.43902\n","[1,   128] train loss: 6.77297\n","[1,   129] train loss: 6.07988\n","[1,   130] train loss: 6.29509\n","[1,   131] train loss: 6.19275\n","[1,   132] train loss: 7.29948\n","[1,   133] train loss: 6.27837\n","[1,   134] train loss: 6.11728\n","[1,   135] train loss: 6.11861\n","[1,   136] train loss: 6.39367\n","[1,   137] train loss: 6.24863\n","[1,   138] train loss: 6.30568\n","[1,   139] train loss: 6.32311\n","[1,   140] train loss: 6.22041\n","[1,   141] train loss: 7.32608\n","[1,   142] train loss: 7.10806\n","[1,   143] train loss: 6.16092\n","[1,   144] train loss: 6.36495\n","[1,   145] train loss: 6.32746\n","[1,   146] train loss: 6.30143\n","[1,   147] train loss: 6.45150\n","[1,   148] train loss: 7.29406\n","[1,   149] train loss: 6.31548\n","[1,   150] train loss: 6.29501\n","[1,   151] train loss: 6.43400\n","[1,   152] train loss: 7.39048\n","[1,   153] train loss: 7.06943\n","[1,   154] train loss: 6.24793\n","[1,   155] train loss: 7.28722\n","[1,   156] train loss: 6.21766\n","[1,   157] train loss: 6.40401\n","[1,   158] train loss: 5.97335\n","[1,   159] train loss: 6.15412\n","[1,   160] train loss: 6.10977\n","[1,   161] train loss: 6.37235\n","[1,   162] train loss: 6.25472\n","[1,   163] train loss: 6.58533\n","[1,   164] train loss: 6.45552\n","[1,   165] train loss: 6.37325\n","[1,   166] train loss: 6.21874\n","[1,   167] train loss: 6.10193\n","[1,   168] train loss: 6.03736\n","[1,   169] train loss: 6.25711\n","[1,   170] train loss: 6.54367\n","[1,   171] train loss: 6.60346\n","[1,   172] train loss: 6.20739\n","[1,   173] train loss: 6.71349\n","[1,   174] train loss: 6.66397\n","[1,   175] train loss: 6.10138\n","[1,   176] train loss: 6.02169\n","[1,   177] train loss: 7.12239\n","[1,   178] train loss: 6.43704\n","[1,   179] train loss: 6.67893\n","[1,   180] train loss: 6.19520\n","[1,   181] train loss: 6.76782\n","[1,   182] train loss: 7.14405\n","[1,   183] train loss: 6.27311\n","[1,   184] train loss: 6.15766\n","[1,   185] train loss: 7.40138\n","[1,   186] train loss: 6.67967\n","[1,   187] train loss: 6.51749\n","[1,   188] train loss: 6.57748\n","[1,   189] train loss: 6.86166\n","[1,   190] train loss: 7.14837\n","[1,   191] train loss: 6.34932\n","[1,   192] train loss: 6.45162\n","[1,   193] train loss: 6.12615\n","[1,   194] train loss: 6.14543\n","[1,   195] train loss: 6.30607\n","[1,   196] train loss: 6.06318\n","[1,   197] train loss: 7.26175\n","[1,   198] train loss: 6.39281\n","[1,   199] train loss: 6.16671\n","[1,   200] train loss: 6.40304\n","In Validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 90/90 [00:10<00:00,  8.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[2       ] validation loss: 6.38378\n","Time elapsed: 0:01:18.733142\n","[1,   201] train loss: 6.15344\n","[1,   202] train loss: 7.19584\n","[1,   203] train loss: 6.38815\n","[1,   204] train loss: 6.27020\n","[1,   205] train loss: 6.11737\n","[1,   206] train loss: 6.14426\n","[1,   207] train loss: 6.25988\n","[1,   208] train loss: 6.42872\n","[1,   209] train loss: 6.50953\n","[1,   210] train loss: 6.03532\n","[1,   211] train loss: 7.10961\n","[1,   212] train loss: 6.04222\n","[1,   213] train loss: 6.01698\n","[1,   214] train loss: 6.31205\n","[1,   215] train loss: 6.63651\n","[1,   216] train loss: 7.54745\n","[1,   217] train loss: 5.91494\n","[1,   218] train loss: 6.31993\n","[1,   219] train loss: 6.21073\n","[1,   220] train loss: 6.14137\n","[1,   221] train loss: 6.21005\n","[1,   222] train loss: 6.33918\n","[1,   223] train loss: 6.26132\n","[1,   224] train loss: 6.08982\n","[1,   225] train loss: 6.68834\n","[1,   226] train loss: 6.11282\n","[1,   227] train loss: 6.69171\n","[1,   228] train loss: 6.41035\n","[1,   229] train loss: 6.98597\n","[1,   230] train loss: 6.22328\n","[1,   231] train loss: 6.89373\n","[1,   232] train loss: 6.11828\n","[1,   233] train loss: 6.30077\n","[1,   234] train loss: 6.08564\n","[1,   235] train loss: 6.28087\n","[1,   236] train loss: 6.29625\n","[1,   237] train loss: 6.19664\n","[1,   238] train loss: 6.12953\n","[1,   239] train loss: 6.51468\n","[1,   240] train loss: 7.25066\n","[1,   241] train loss: 6.37245\n","[1,   242] train loss: 6.51767\n","[1,   243] train loss: 7.47057\n","[1,   244] train loss: 7.41722\n","[1,   245] train loss: 6.50479\n","[1,   246] train loss: 6.17911\n","[1,   247] train loss: 6.22901\n","[1,   248] train loss: 7.06450\n","[1,   249] train loss: 6.42287\n","[1,   250] train loss: 6.72374\n","[1,   251] train loss: 6.92163\n","[1,   252] train loss: 6.87633\n","[1,   253] train loss: 6.67806\n","[1,   254] train loss: 6.05208\n","[1,   255] train loss: 5.93132\n","[1,   256] train loss: 5.95961\n","[1,   257] train loss: 6.41697\n","[1,   258] train loss: 6.54430\n","[1,   259] train loss: 6.24774\n","[1,   260] train loss: 6.28621\n","[1,   261] train loss: 7.41704\n","[1,   262] train loss: 6.30238\n","[1,   263] train loss: 6.54746\n","[1,   264] train loss: 7.11005\n","[1,   265] train loss: 6.43261\n","[1,   266] train loss: 6.31504\n","[1,   267] train loss: 6.41474\n","[1,   268] train loss: 7.31311\n","[1,   269] train loss: 6.60073\n","[1,   270] train loss: 6.35636\n","[1,   271] train loss: 6.97011\n","[1,   272] train loss: 6.05994\n","[1,   273] train loss: 6.07897\n","[1,   274] train loss: 6.31767\n","[1,   275] train loss: 7.17532\n","[1,   276] train loss: 6.06674\n","[1,   277] train loss: 6.39399\n","[1,   278] train loss: 6.55078\n","[1,   279] train loss: 7.25894\n","[1,   280] train loss: 5.92455\n","[1,   281] train loss: 6.38455\n","[1,   282] train loss: 6.25796\n","[1,   283] train loss: 6.17728\n","[1,   284] train loss: 6.37919\n","[1,   285] train loss: 6.29999\n","[1,   286] train loss: 6.14104\n","[1,   287] train loss: 6.05809\n","[1,   288] train loss: 5.93829\n","[1,   289] train loss: 6.13328\n","[1,   290] train loss: 6.31464\n","[1,   291] train loss: 6.38467\n","[1,   292] train loss: 6.51130\n","[1,   293] train loss: 6.03037\n","[1,   294] train loss: 7.38585\n","[1,   295] train loss: 6.35875\n","[1,   296] train loss: 7.21271\n","[1,   297] train loss: 6.50849\n","[1,   298] train loss: 6.36873\n","[1,   299] train loss: 6.36240\n","[1,   300] train loss: 6.65928\n","In Validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 90/90 [00:10<00:00,  8.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[3       ] validation loss: 6.36931\n","Time elapsed: 0:01:59.992380\n","[1,   301] train loss: 7.34285\n","[1,   302] train loss: 6.29071\n","[1,   303] train loss: 6.63292\n","[1,   304] train loss: 6.42075\n","[1,   305] train loss: 6.61776\n","[1,   306] train loss: 6.29215\n","[1,   307] train loss: 6.49406\n","[1,   308] train loss: 6.20427\n","[1,   309] train loss: 5.99798\n","[1,   310] train loss: 7.16083\n","[1,   311] train loss: 6.11954\n","[1,   312] train loss: 6.32473\n","[1,   313] train loss: 6.37439\n","[1,   314] train loss: 6.40355\n","[1,   315] train loss: 6.90515\n","[1,   316] train loss: 6.43295\n","[1,   317] train loss: 6.10808\n","[1,   318] train loss: 7.18850\n","[1,   319] train loss: 6.16620\n","[1,   320] train loss: 6.01378\n","[1,   321] train loss: 6.19963\n","[1,   322] train loss: 6.27670\n","[1,   323] train loss: 6.30661\n","[1,   324] train loss: 6.62276\n","[1,   325] train loss: 6.23211\n","[1,   326] train loss: 7.05656\n","[1,   327] train loss: 7.43256\n","[1,   328] train loss: 6.30725\n","[1,   329] train loss: 6.25284\n","[1,   330] train loss: 6.04700\n","[1,   331] train loss: 6.32466\n","[1,   332] train loss: 6.81066\n","[1,   333] train loss: 6.27109\n","[1,   334] train loss: 6.40262\n","[1,   335] train loss: 6.61905\n","[1,   336] train loss: 6.77625\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-0600403159a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mmodel_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_with_parallel_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/SAWorks.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model/model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/model.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-0600403159a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_obj, train_loader, val_loader, actual_batch_size, epochs, base_lr, weight_decay, scheduler, warmup_steps, model_save_name, save_model_freq, val_freq, dropout, write_logs, logs_folder, delete_logs, load_existing_model)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mseq_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%d, %5d] train loss: %.5f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrite_logs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from torch.utils.tensorboard import SummaryWriter\n","import argparse\n","import yaml\n","import os\n","import time, datetime\n","from tqdm import tqdm\n","import transformers\n","import random\n","import sys\n","from lib.evaluate import evaluate\n","from lib.select_model import select_model\n","from lib.utils import *\n","from parameter_efficient.parallel_adapter import Model_with_parallel_adapter\n","from fine_tuning.ULMFiT import Model_with_ULMFiT\n","\n","\n","\n","def train(\n","    model_obj = None,\n","    train_loader = None,\n","    val_loader = None,\n","    actual_batch_size = 15,\n","    epochs = 20,\n","    base_lr = 0.001,\n","    weight_decay = 0.001,\n","    scheduler = None,\n","    warmup_steps = 5000,\n","    model_save_name = None,\n","    save_model_freq = 300,\n","    val_freq = 100,\n","    dropout = 0.1,\n","    write_logs = True,\n","    logs_folder = 'runs',\n","    delete_logs = True,\n","    load_existing_model = True,\n","    ):\n","  \n","\n","    if model_obj is None:\n","        raise Exception(\"Model object was not provided.\")\n","    if train_loader is None or val_loader is None:\n","        raise Exception(\"train_loader or val_loader were not provided.\")\n","    if model_save_name is None:\n","        raise Exception(\"Path to directory for saving model was not provided.\")\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    base_model = model_obj.base_model\n","    model_size = model_obj.model_size\n","\n","    if hasattr(model_obj, 'model'):\n","        model = model_obj.model\n","    else:\n","        model = model_obj\n","\n","    if write_logs:\n","        writer = SummaryWriter(logs_folder)\n","\n","    if delete_logs:\n","        # Delete all the logs\n","        for root, dirs, files in os.walk(logs_folder):\n","            for file in files:\n","                os.remove(os.path.join(root, file))\n","        print(\"Logs deleted\")\n","\n","    if load_existing_model:\n","        state_dict = torch.load(model_save_name)\n","        model.load_state_dict(state_dict)\n","        print(\"Existing model loaded\")\n","\n","    model = model.to(device)\n","\n","    optimizer = get_optimizer(model, model_obj, base_lr, weight_decay)\n","    scheduler = select_scheduler(scheduler, model_obj.technique, optimizer, len(train_loader), epochs, actual_batch_size, warmup_steps)\n","\n","    model.train()\n","    model_obj.model = model\n","\n","    seq_count = 0\n","    start = time.time()\n","\n","\n","    for epoch in range(epochs):\n","\n","        print(f\"EPOCH {epoch} started\" + '=' * 30)\n","\n","        for train_counter, train_batch in enumerate(train_loader, 0):\n","\n","            tokens = train_batch.to(device)\n","            tokens = process_tokens(tokens, device, model_obj.technique)\n","            outputs = model(tokens, labels=tokens)\n","            loss = outputs[0]\n","            loss.backward()\n","            seq_count += 1\n","\n","            print('[%d, %5d] train loss: %.5f' % (epoch + 1, seq_count, loss.detach().data))\n","            if write_logs:\n","                writer.add_scalar(\"train_loss\", float(loss.detach().data), seq_count)\n","\n","            # Resorting to this approach as processing input sequences in \n","            # parallel would often cause 'CUDA out of memory'\n","            if seq_count % actual_batch_size == 0:\n","              optimizer.step()\n","              scheduler.step()\n","              optimizer.zero_grad()\n","              model.zero_grad()\n","              model = batch_routine(model, model_obj)\n","\n","            if seq_count % save_model_freq == 0:\n","                torch.save(model.state_dict(), model_save_name)\n","\n","            if seq_count % val_freq == 0:\n","                validate(model, model_obj, val_loader, device, seq_count // val_freq, writer)\n","                print(\"Time elapsed:\", str(datetime.timedelta(seconds=time.time() - start)))\n","                model.train()\n","\n","            if hasattr(model_obj, 'model'):\n","                model_obj.model = model\n","\n","            model = modify(model, model_obj, seq_count)\n","            \n","\n","            \n","\n","\n","def validate(model, model_obj, val_loader, device, batch_count, writer):\n","\n","    model.eval()\n","    counter = 0\n","    with torch.no_grad():\n","        running_loss = 0\n","\n","        for val_counter, val_batch in enumerate(tqdm(val_loader), 0):\n","\n","            tokens = val_batch.to(device)\n","            tokens = process_tokens(tokens, device, model_obj.technique)\n","            outputs = model(tokens, labels=tokens)\n","            loss = outputs[0]\n","            counter += 1\n","            running_loss += loss\n","\n","        print('[%d       ] validation loss: %.5f' % (batch_count,\n","                                                     running_loss / len(val_loader)))\n","        writer.add_scalar('val loss', running_loss / len(val_loader),\n","                          batch_count)\n","\n","\n","\n","\n","\n","\n","def arg_parser():\n","\n","    with open('config.yml', 'r') as file:\n","        args = yaml.safe_load(file)\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--batch_size', type=int, default=args['batch_size'], help='Batch size for dataloaders')\n","    parser.add_argument('--actual_batch_len', type=int, default=args['actual_batch_len'], help='Actual batch size')\n","    parser.add_argument('--num_workers', type=int, default=args['num_workers'], help='Number of workers')\n","    parser.add_argument('--warmup_steps', type=int, default=args['warmup_steps'], help='Warmup steps')\n","    parser.add_argument('--lr', type=int, default=args['base_lr'], help='Learning rate')\n","    parser.add_argument('--reset_tb', type=bool, default=args['reset_tb'], help='Reset tensorboard')\n","    parser.add_argument('--write_logs', type=bool, default=args['write_logs'], help='Write logs to tensorboard')\n","    parser.add_argument('--epochs', type=int, default=args['epochs'], help='Number of epochs of training')\n","    parser.add_argument('--unfreeze_qty', type=int, default=args['unfreeze_qty'], help='Number of layers to unfreeze at a time')\n","    parser.add_argument('--dlr_factor', type=int, default=args['dlr_factor'], help='Discriminative learning rate decay factor')\n","    parser.add_argument('--base_model', type=str, default=args['base_model'], help='Base model')\n","    parser.add_argument('--model', type=str, default=args['model'], help='Model name')\n","    parser.add_argument('--dataset_path', type=str, default=args['dataset_path'], help='Relative path to the dataset')\n","    parser.add_argument('--model_size', type=str, default=args['model_size'], help=\"Model size options: '', 'medium', 'large'\")\n","    parser.add_argument('--weight_decay', type=int, default=args['weight_decay'], help='Weight decay coefficient')\n","    parser.add_argument('--dropout', type=int, default=args['dropout'], help='Dropout rate')\n","    parser.add_argument('--unfreeze_freq', type=int, default=args['unfreeze_freq'], help='Frequency of unfreezing model layers')\n","    parser.add_argument('--scheduler', type=str, default=args['scheduler'], help='Name of the scheduler')\n","    parser.add_argument('--gradual_unfreezing', type=bool, default=args['gradual_unfreezing'], help='Gradual unfreezing switch')\n","    parser.add_argument('--chain_thaw', type=bool, default=args['chain_thaw'], help='Chain thaw switch')\n","    parser.add_argument('--apply_dlr', type=bool, default=args['apply_dlr'], help='Apply discriminative learning rate')\n","    parser.add_argument('--biases_only', type=bool, default=args['biases_only'], help='Fine tune bias parameters only')\n","    parser.add_argument('--block_size', type=int, default=args['block_size'], help='Tokenizer block size')\n","    parser.add_argument('--freeze_init_layers', type=int, default=args['freeze_init_layers'], help='Freeze first n layers')\n","    parser.add_argument('--freeze_nth_layer', type=int, default=args['freeze_nth_layer'], help='Freeze nth layer') \n","    parser.add_argument('--val_freq', type=int, default=args['val_freq'], help='Number of training steps between validation')\n","    parser.add_argument('--eval_model_freq', type=int, default=args['eval_model_freq'], help='Evaluate model frequency')\n","    parser.add_argument('--save_model_freq', type=int, default=args['save_model_freq'], help='Log frequency')\n","    parser.add_argument('--gen_max_len', type=int, default=args['gen_max_len'], help='Maximum length of output generated')\n","    parser.add_argument('--load_existing_model', type=int, default=args['load_existing_model'], help='(bool) load the existing model')\n","    parser.add_argument('--percent_of_val', type=int, default=args['percent_of_val'],\n","                        help='percentage of the validation set that will '\n","                             'be evaluated, type int')\n","    parser.add_argument('--model_save_name', type=str, default=args['model_save_name'], help='Saved model name')\n","    parser.add_argument('--logs_base_run', type=str, default=args['logs_base_run'], help='Tensorboard logs base folder name')\n","    parser.add_argument('--test_split', type=float, default=args['test_split'], help='Test split ratio (from val set)')\n","    parser.add_argument('--train_split', type=float, default=args['train_split'], help='Train split ratio')\n","    parser.add_argument('--test_split_token', type=str, default=args['test_split_token'], help='Token to split input into input and output for test set')\n","    parser.add_argument('--test_end_token', type=str, default=args['test_end_token'], help='Token at the end of input/output pair')\n","    \n","\n","    return parser\n","\n","\n","\n","if __name__ == \"__main__\":\n","\n","    model_obj = Model_with_parallel_adapter()\n","    train_loader, val_loader, test_loader = load_dataloaders(dataset_path = 'data/SAWorks.txt')\n","    train(model_obj, train_loader, val_loader, model_save_name='model/model.pth')\n","    metrics = evaluate(test_loader, model_obj, None, None, 'model/model.pth', 100)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1399,"status":"ok","timestamp":1669373814949,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"},"user_tz":-330},"id":"9YYjrpN_zAza","outputId":"c7b8f5b2-5fb8-43a7-8d7b-b609c715a121"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialized empty Git repository in /content/gdrive/MyDrive/TransferLearningToolkit/.git/\n"]}],"source":["!git init"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFxTyRq8BOEo"},"outputs":[],"source":["!git config --global user.email \"vibhud04@gmail.com\"\n","!git config --global user.name \"Vibhu04\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1213,"status":"ok","timestamp":1669294187477,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"},"user_tz":-330},"id":"dBb6WtU9_0Tv","outputId":"c39c5056-2b2a-4059-9e21-8f04d168d231"},"outputs":[{"name":"stdout","output_type":"stream","text":["On branch master\n","nothing to commit, working tree clean\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf2aUtpI_7cl"},"outputs":[],"source":["!git add -A"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3360,"status":"ok","timestamp":1669374183705,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"},"user_tz":-330},"id":"w0LaChDjADw6","outputId":"fb027f76-5031-42b7-c925-22a847f0ebf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[master (root-commit) c5584a6] Added comments, cleaned up code\n"," 98 files changed, 7131 insertions(+)\n"," create mode 100644 .idea/.gitignore\n"," create mode 100644 .idea/TransferLearningToolkit.iml\n"," create mode 100644 .idea/inspectionProfiles/profiles_settings.xml\n"," create mode 100644 .idea/misc.xml\n"," create mode 100644 .idea/modules.xml\n"," create mode 100644 __pycache__/gpt2_with_adapter.cpython-38.pyc\n"," create mode 100644 __pycache__/main.cpython-38.pyc\n"," create mode 100644 __pycache__/select_model.cpython-37.pyc\n"," create mode 100644 __pycache__/utils.cpython-37.pyc\n"," create mode 100644 config.yml\n"," create mode 100644 data/SAWorks.txt\n"," create mode 100644 data/__pycache__/dataset.cpython-37.pyc\n"," create mode 100644 data/__pycache__/dataset.cpython-38.pyc\n"," create mode 100644 data/dataset.py\n"," create mode 100644 fine_tuning/__pycache__/stlr.cpython-37.pyc\n"," create mode 100644 fine_tuning/__pycache__/utils.cpython-37.pyc\n"," create mode 100644 fine_tuning/stlr.py\n"," create mode 100644 fine_tuning/utils.py\n"," create mode 100644 lib/TransferLearningToolkit.ipynb\n"," create mode 100644 lib/__pycache__/evaluate.cpython-37.pyc\n"," create mode 100644 lib/__pycache__/select_model.cpython-37.pyc\n"," create mode 100644 lib/__pycache__/utils.cpython-37.pyc\n"," create mode 100644 lib/evaluate.py\n"," create mode 100644 lib/select_model.py\n"," create mode 100644 lib/utils.py\n"," create mode 100644 parameter_efficient/__pycache__/adapter.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/adapter_bapna.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/adapterdrop.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_adapter.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_adapter_bapna.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_adapterdrop.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_lora.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_mam_adapter.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_parallel_adapter.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_prefix_tuning.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/gpt2_with_prompt_tuning.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/lora.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/mam_adapter.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/parallel_adapter.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/prefix_tuning.cpython-37.pyc\n"," create mode 100644 parameter_efficient/__pycache__/prompt_tuning.cpython-37.pyc\n"," create mode 100644 parameter_efficient/adapter.py\n"," create mode 100644 parameter_efficient/adapter_bapna.py\n"," create mode 100644 parameter_efficient/adapterdrop.py\n"," create mode 100644 parameter_efficient/lora.py\n"," create mode 100644 parameter_efficient/mam_adapter.py\n"," create mode 100644 parameter_efficient/masking.py\n"," create mode 100644 parameter_efficient/parallel_adapter.py\n"," create mode 100644 parameter_efficient/prefix_tuning.py\n"," create mode 100644 parameter_efficient/prompt_tuning.py\n"," create mode 100644 runs/events.out.tfevents.1669200593.0f0edb985e1e.1596.2\n"," create mode 100644 runs/events.out.tfevents.1669202443.0f0edb985e1e.2265.0\n"," create mode 100644 runs/events.out.tfevents.1669204630.0f0edb985e1e.2806.0\n"," create mode 100644 runs/events.out.tfevents.1669208018.0f0edb985e1e.3985.0\n"," create mode 100644 runs/events.out.tfevents.1669208158.0f0edb985e1e.3985.1\n"," create mode 100644 runs/events.out.tfevents.1669208489.0f0edb985e1e.4353.0\n"," create mode 100644 runs/events.out.tfevents.1669208689.0f0edb985e1e.4693.0\n"," create mode 100644 runs/events.out.tfevents.1669217289.423aab11fbf2.1325.0\n"," create mode 100644 runs/events.out.tfevents.1669217518.1d732dab443f.303.0\n"," create mode 100644 runs/events.out.tfevents.1669218215.1d732dab443f.595.0\n"," create mode 100644 runs/events.out.tfevents.1669218539.1d732dab443f.951.0\n"," create mode 100644 runs/events.out.tfevents.1669219207.1d732dab443f.1336.0\n"," create mode 100644 runs/events.out.tfevents.1669271739.09be04090813.328.0\n"," create mode 100644 runs/events.out.tfevents.1669272861.09be04090813.784.0\n"," create mode 100644 runs/events.out.tfevents.1669273390.09be04090813.1146.0\n"," create mode 100644 runs/events.out.tfevents.1669274320.09be04090813.1538.0\n"," create mode 100644 runs/events.out.tfevents.1669275161.09be04090813.1916.0\n"," create mode 100644 runs/events.out.tfevents.1669276490.09be04090813.2668.0\n"," create mode 100644 runs/events.out.tfevents.1669276629.09be04090813.2986.0\n"," create mode 100644 runs/events.out.tfevents.1669276848.09be04090813.3318.0\n"," create mode 100644 runs/events.out.tfevents.1669277613.09be04090813.3720.0\n"," create mode 100644 runs/events.out.tfevents.1669277925.09be04090813.4084.0\n"," create mode 100644 runs/events.out.tfevents.1669278126.09be04090813.4435.0\n"," create mode 100644 runs/events.out.tfevents.1669278402.09be04090813.4791.0\n"," create mode 100644 runs/events.out.tfevents.1669278596.09be04090813.5138.0\n"," create mode 100644 runs/events.out.tfevents.1669278785.09be04090813.5481.0\n"," create mode 100644 runs/events.out.tfevents.1669280419.eaa84158048e.77.0\n"," create mode 100644 runs/events.out.tfevents.1669280670.eaa84158048e.501.0\n"," create mode 100644 runs/events.out.tfevents.1669281320.eaa84158048e.888.0\n"," create mode 100644 runs/events.out.tfevents.1669283357.48f08a50d643.773.0\n"," create mode 100644 runs/events.out.tfevents.1669283918.48f08a50d643.1175.0\n"," create mode 100644 runs/events.out.tfevents.1669284643.48f08a50d643.1615.0\n"," create mode 100644 runs/events.out.tfevents.1669286347.48f08a50d643.2444.0\n"," create mode 100644 runs/events.out.tfevents.1669287342.48f08a50d643.2444.1\n"," create mode 100644 runs/events.out.tfevents.1669287563.48f08a50d643.2444.2\n"," create mode 100644 runs/events.out.tfevents.1669287983.48f08a50d643.2982.0\n"," create mode 100644 runs/events.out.tfevents.1669288156.48f08a50d643.2982.1\n"," create mode 100644 runs/events.out.tfevents.1669288230.48f08a50d643.3310.0\n"," create mode 100644 runs/events.out.tfevents.1669289993.cc6bed1b2007.499.0\n"," create mode 100644 runs/events.out.tfevents.1669290216.cc6bed1b2007.829.0\n"," create mode 100644 runs/events.out.tfevents.1669291135.cc6bed1b2007.1288.0\n"," create mode 100644 runs/events.out.tfevents.1669291375.cc6bed1b2007.1635.0\n"," create mode 100644 runs/events.out.tfevents.1669292147.cc6bed1b2007.1635.1\n"," create mode 100644 runs/events.out.tfevents.1669292310.cc6bed1b2007.1635.2\n"," create mode 100644 runs/events.out.tfevents.1669292688.cc6bed1b2007.2168.0\n"," create mode 100644 runs/events.out.tfevents.1669292923.cc6bed1b2007.2504.0\n"," create mode 100644 runs/events.out.tfevents.1669357246.4b152ce55959.1631.0\n"," create mode 100644 train.py\n"]}],"source":["!git commit -m \"Added comments, cleaned up code\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oy50hGLFeW2"},"outputs":[],"source":["!git remote add origin https://Vibhu04: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@github.com/Vibhu04/PLM-Transfer-Learning-Toolkit.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2798,"status":"ok","timestamp":1669374203715,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"},"user_tz":-330},"id":"yJKbPdFRAr86","outputId":"ff54f3a8-6f64-43f7-a462-0c362a9138e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Counting objects: 112, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (103/103), done.\n","Writing objects: 100% (112/112), 3.86 MiB | 2.91 MiB/s, done.\n","Total 112 (delta 15), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (15/15), done.\u001b[K\n","To https://github.com/Vibhu04/PLM-Transfer-Learning-Toolkit.git\n"," + 9054834...c5584a6 master -> master (forced update)\n"]}],"source":["!git push origin master --force"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1269,"status":"ok","timestamp":1669120531534,"user":{"displayName":"Vibhu Dalal","userId":"11849916197958116802"},"user_tz":-330},"id":"dS3DfNTGCpnU","outputId":"c07b8751-8140-485a-b2e6-7c2073726214"},"outputs":[{"name":"stdout","output_type":"stream","text":["warning: no common commits\n","remote: Enumerating objects: 58, done.\u001b[K\n","remote: Counting objects:   1% (1/58)\u001b[K\rremote: Counting objects:   3% (2/58)\u001b[K\rremote: Counting objects:   5% (3/58)\u001b[K\rremote: Counting objects:   6% (4/58)\u001b[K\rremote: Counting objects:   8% (5/58)\u001b[K\rremote: Counting objects:  10% (6/58)\u001b[K\rremote: Counting objects:  12% (7/58)\u001b[K\rremote: Counting objects:  13% (8/58)\u001b[K\rremote: Counting objects:  15% (9/58)\u001b[K\rremote: Counting objects:  17% (10/58)\u001b[K\rremote: Counting objects:  18% (11/58)\u001b[K\rremote: Counting objects:  20% (12/58)\u001b[K\rremote: Counting objects:  22% (13/58)\u001b[K\rremote: Counting objects:  24% (14/58)\u001b[K\rremote: Counting objects:  25% (15/58)\u001b[K\rremote: Counting objects:  27% (16/58)\u001b[K\rremote: Counting objects:  29% (17/58)\u001b[K\rremote: Counting objects:  31% (18/58)\u001b[K\rremote: Counting objects:  32% (19/58)\u001b[K\rremote: Counting objects:  34% (20/58)\u001b[K\rremote: Counting objects:  36% (21/58)\u001b[K\rremote: Counting objects:  37% (22/58)\u001b[K\rremote: Counting objects:  39% (23/58)\u001b[K\rremote: Counting objects:  41% (24/58)\u001b[K\rremote: Counting objects:  43% (25/58)\u001b[K\rremote: Counting objects:  44% (26/58)\u001b[K\rremote: Counting objects:  46% (27/58)\u001b[K\rremote: Counting objects:  48% (28/58)\u001b[K\rremote: Counting objects:  50% (29/58)\u001b[K\rremote: Counting objects:  51% (30/58)\u001b[K\rremote: Counting objects:  53% (31/58)\u001b[K\rremote: Counting objects:  55% (32/58)\u001b[K\rremote: Counting objects:  56% (33/58)\u001b[K\rremote: Counting objects:  58% (34/58)\u001b[K\rremote: Counting objects:  60% (35/58)\u001b[K\rremote: Counting objects:  62% (36/58)\u001b[K\rremote: Counting objects:  63% (37/58)\u001b[K\rremote: Counting objects:  65% (38/58)\u001b[K\rremote: Counting objects:  67% (39/58)\u001b[K\rremote: Counting objects:  68% (40/58)\u001b[K\rremote: Counting objects:  70% (41/58)\u001b[K\rremote: Counting objects:  72% (42/58)\u001b[K\rremote: Counting objects:  74% (43/58)\u001b[K\rremote: Counting objects:  75% (44/58)\u001b[K\rremote: Counting objects:  77% (45/58)\u001b[K\rremote: Counting objects:  79% (46/58)\u001b[K\rremote: Counting objects:  81% (47/58)\u001b[K\rremote: Counting objects:  82% (48/58)\u001b[K\rremote: Counting objects:  84% (49/58)\u001b[K\rremote: Counting objects:  86% (50/58)\u001b[K\rremote: Counting objects:  87% (51/58)\u001b[K\rremote: Counting objects:  89% (52/58)\u001b[K\rremote: Counting objects:  91% (53/58)\u001b[K\rremote: Counting objects:  93% (54/58)\u001b[K\rremote: Counting objects:  94% (55/58)\u001b[K\rremote: Counting objects:  96% (56/58)\u001b[K\rremote: Counting objects:  98% (57/58)\u001b[K\rremote: Counting objects: 100% (58/58)\u001b[K\rremote: Counting objects: 100% (58/58), done.\u001b[K\n","remote: Compressing objects: 100% (51/51), done.\u001b[K\n","remote: Total 58 (delta 5), reused 58 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (58/58), done.\n","From https://github.com/Vibhu04/PLM-Transfer-Learning-Toolkit\n"," * branch            master     -> FETCH_HEAD\n"," * [new branch]      master     -> origin/master\n","fatal: refusing to merge unrelated histories\n"]}],"source":["!git pull origin master"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1G3Hnu4VZEJLPwLHeyPTUSuM1JuivfTVq","authorship_tag":"ABX9TyMQZj3epQ+AhhO27WZYXL+E"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}